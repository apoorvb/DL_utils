{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Emotion_det.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDU15KYazl-9"
      },
      "source": [
        "# MNIST\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBa2R_bBzti3"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.datasets as datasets \n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWqKzbpDKAQN"
      },
      "source": [
        "pip install -U tensorboard-plugin-profile\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKEEVFN9pPvt"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjrKWd6tdMZx"
      },
      "source": [
        "train_dataset = datasets.MNIST(\n",
        "    root=\"dataset/\", train=True, transform=transforms.ToTensor(), download=True\n",
        ")\n",
        "test_dataset = datasets.MNIST(\n",
        "    root=\"dataset/\", train=False, transform=transforms.ToTensor(), download=True\n",
        ")\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=128, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClkiGAgpdUl2"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, in_channels=1, num_classes=10):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels=1,\n",
        "            out_channels=8,\n",
        "            kernel_size=(3, 3),\n",
        "            stride=(1, 1),\n",
        "            padding=(1, 1),\n",
        "        )\n",
        "        self.pool = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            in_channels=8,\n",
        "            out_channels=16,\n",
        "            kernel_size=(3, 3),\n",
        "            stride=(1, 1),\n",
        "            padding=(1, 1),\n",
        "        )\n",
        "        self.fc1 = nn.Linear(16 * 7 * 7, num_classes)\n",
        "        torch.nn.init.xavier_uniform(self.conv1.weight)\n",
        "        torch.nn.init.xavier_uniform(self.conv2.weight)\n",
        "        torch.nn.init.xavier_uniform(self.fc1.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        x = self.fc1(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ER4KKrmoljq_"
      },
      "source": [
        "in_channel = 1\n",
        "num_classes = 10\n",
        "num_epoch = 8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6vahj7sm0MI"
      },
      "source": [
        "model = CNN().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nR0-IF4gYbW"
      },
      "source": [
        "batch_sizes = [64,128,256]\n",
        "l_rates = [0.1, 0.01, 0.001]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-aBy4mglyQX"
      },
      "source": [
        "for batch in batchs_sizes:\n",
        "  for l_rate in l_rates:\n",
        "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch, shuffle=True)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimise = optim.Adam(model.parameters(), lr = l_rate)\n",
        "    writer = SummaryWriter('runs/MNIST/ Batch size {}, L_rate {}'.format(batch, l_rate))\n",
        "    step = 0\n",
        "    for epoch in range(num_epoch):\n",
        "      losses = []\n",
        "      for batch_id,(data, target) in tqdm(enumerate(train_loader), total=len(train_loader),\n",
        "                                          position = 0, leave = False):\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        optimise.zero_grad()\n",
        "        loss.backward()\n",
        "        optimise.step()\n",
        "\n",
        "        losses.append(loss.item())\n",
        "        _, correct_test = output.max(1)\n",
        "        num_correct = (correct_test == target).sum()\n",
        "        running_train_acc = float(num_correct)/float(data.shape[0])\n",
        "        \n",
        "        ''' writes scalars and plots histogram for every forward pass'''\n",
        "        writer.add_histogram('layer1', model.conv1.weight)\n",
        "        writer.add_histogram('layer2', model.conv2.weight)\n",
        "        writer.add_histogram('layer3', model.fc1.weight)\n",
        "        writer.add_scalar('training_loss', loss, global_step = step)\n",
        "        writer.add_scalar('training_acc', running_train_acc, global_step = step)    \n",
        "        step += 1\n",
        "      \n",
        "      ''' Helps in choosing best combination of hyperparameters. '''\n",
        "      writer.add_hparams({'lr': l_rate, 'b_size': batch, 'loss':sum(losses)/len(losses)}) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMq4G0E5opGT"
      },
      "source": [
        "%tensorboard --logdir runs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vWa7KYVoph1",
        "outputId": "c4a2e4b0-fc92-49b5-cbef-648d169127dd"
      },
      "source": [
        "model.eval()\n",
        "num_classes = 10\n",
        "num_correct = 0\n",
        "num_samples = 0\n",
        "confusion_matrix = torch.zeros(num_classes, num_classes)\n",
        "counter = 0\n",
        "with torch.no_grad():\n",
        "  for y_hat, y in test_loader:\n",
        "    counter += 1\n",
        "    y_hat = y_hat.to(device)\n",
        "    y = y.to(device)\n",
        "    pred = model(y_hat)\n",
        "    _, correct_test = pred.max(1)\n",
        "    num_correct += (correct_test == y).sum()\n",
        "    num_samples += correct_test.size(0)\n",
        "    for t, p in zip(y.view(-1), correct_test.view(-1)):\n",
        "           confusion_matrix[t, p] += 1\n",
        "  print('acc : {}'.format(num_correct/num_samples))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "acc : 0.9827999472618103\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEmpFcw7ksuo"
      },
      "source": [
        "print(confusion_matrix) # num_classes*num_classes\n",
        "print(confusion_matrix.diag()/confusion_matrix.sum(1)) # TP/TP+FP - Precision for every class"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}